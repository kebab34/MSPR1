# MSPR TPRE501 - Projet de Mise en Situation Professionnelle

## ğŸ“‹ Description du projet

Projet MSPR dÃ©veloppÃ© avec une architecture moderne utilisant :
- **Base de donnÃ©es** : Supabase (PostgreSQL)
- **API** : FastAPI avec documentation OpenAPI automatique
- **Interface Admin** : Streamlit
- **ETL** : Python + Pandas + SQLAlchemy
- **Orchestration** : Docker & Docker Compose

## ğŸ—ï¸ Architecture

```
MSPR1/
â”œâ”€â”€ api/                    # Service FastAPI
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py        # Point d'entrÃ©e FastAPI
â”‚   â”‚   â”œâ”€â”€ core/          # Configuration et base de donnÃ©es
â”‚   â”‚   â”œâ”€â”€ api/           # Routes et endpoints
â”‚   â”‚   â”œâ”€â”€ models/        # ModÃ¨les de donnÃ©es
â”‚   â”‚   â”œâ”€â”€ schemas/       # SchÃ©mas Pydantic
â”‚   â”‚   â””â”€â”€ utils/         # Utilitaires
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ streamlit/             # Interface d'administration
â”‚   â”œâ”€â”€ app.py            # Application principale
â”‚   â”œâ”€â”€ pages/            # Pages Streamlit
â”‚   â”œâ”€â”€ utils/            # Utilitaires
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ etl/                   # Pipeline ETL
â”‚   â”œâ”€â”€ extract.py        # Extraction des donnÃ©es
â”‚   â”œâ”€â”€ transform.py      # Transformation des donnÃ©es
â”‚   â”œâ”€â”€ load.py           # Chargement dans Supabase
â”‚   â”œâ”€â”€ scheduler.py      # Planificateur ETL
â”‚   â”œâ”€â”€ data/             # DonnÃ©es sources
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ docker-compose.yml     # Configuration Docker
â”œâ”€â”€ .env.example          # Exemple de variables d'environnement
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸš€ DÃ©marrage rapide

### PrÃ©requis

- Docker et Docker Compose installÃ©s
- Compte Supabase crÃ©Ã©
- Variables d'environnement configurÃ©es

### Installation

1. **Cloner le projet** (si applicable)
   ```bash
   git clone <repository-url>
   cd MSPR1
   ```

2. **Configurer les variables d'environnement**
   ```bash
   cp .env.example .env
   ```
   
   Ã‰ditez le fichier `.env` et remplissez vos credentials Supabase :
   ```env
   SUPABASE_URL=https://your-project.supabase.co
   SUPABASE_KEY=your-anon-key
   SUPABASE_SERVICE_KEY=your-service-role-key
   DATABASE_URL=postgresql://postgres:password@db.your-project.supabase.co:5432/postgres
   JWT_SECRET=your-jwt-secret-key
   ETL_SCHEDULE=0 */6 * * *
   ```

3. **Construire et dÃ©marrer les services**
   ```bash
   docker-compose up --build
   ```

4. **AccÃ©der aux services**
   - **API FastAPI** : http://localhost:8000
   - **Documentation API** : http://localhost:8000/docs
   - **Interface Streamlit** : http://localhost:8501

## ğŸ“š Documentation des services

### API FastAPI

L'API FastAPI est accessible sur le port 8000. La documentation interactive est disponible Ã  `/docs`.

#### Endpoints disponibles

- `GET /` - Page d'accueil
- `GET /health` - VÃ©rification de santÃ©
- `GET /api/v1/health` - Health check API
- `GET /api/v1/example` - Exemple d'endpoint

#### Ajouter un nouvel endpoint

1. CrÃ©er un nouveau fichier dans `api/app/api/v1/endpoints/`
2. CrÃ©er le router avec vos endpoints
3. Ajouter le router dans `api/app/api/v1/api.py`

### Interface Streamlit

L'interface d'administration Streamlit est accessible sur le port 8501.

#### Structure des pages

- **Accueil** : Page principale avec vue d'ensemble
- **Dashboard** : Tableaux de bord et visualisations
- **Configuration** : ParamÃ¨tres de l'application

#### Ajouter une nouvelle page

1. CrÃ©er un nouveau fichier dans `streamlit/pages/`
2. Utiliser les widgets Streamlit pour crÃ©er l'interface
3. Utiliser `utils/api_client.py` pour communiquer avec l'API

### Pipeline ETL

Le pipeline ETL s'exÃ©cute automatiquement selon le planning dÃ©fini dans `ETL_SCHEDULE`.

#### Structure ETL

1. **Extract** (`extract.py`) : Extraction des donnÃ©es depuis diverses sources
2. **Transform** (`transform.py`) : Transformation et nettoyage des donnÃ©es
3. **Load** (`load.py`) : Chargement des donnÃ©es dans Supabase

#### Personnaliser le pipeline

Ã‰ditez la fonction `run_etl_pipeline()` dans `etl/scheduler.py` pour dÃ©finir votre processus ETL.

#### ExÃ©cution manuelle

Pour tester le pipeline ETL manuellement :
```bash
docker-compose exec etl python scheduler.py
```

## ğŸ› ï¸ DÃ©veloppement

### Structure du code

- **API** : Architecture modulaire avec sÃ©paration des routes, modÃ¨les et utilitaires
- **Streamlit** : Application multi-pages avec utilitaires rÃ©utilisables
- **ETL** : Pipeline modulaire Extract-Transform-Load

### Ajout de dÃ©pendances

1. Ajouter la dÃ©pendance dans le `requirements.txt` appropriÃ©
2. Reconstruire le conteneur Docker :
   ```bash
   docker-compose build --no-cache <service>
   docker-compose up <service>
   ```

### Tests

Les tests peuvent Ãªtre ajoutÃ©s dans chaque service :
- `api/tests/` pour les tests de l'API
- `streamlit/tests/` pour les tests Streamlit
- `etl/tests/` pour les tests ETL

## ğŸ“ Variables d'environnement

| Variable | Description | Exemple |
|----------|-------------|---------|
| `SUPABASE_URL` | URL de votre projet Supabase | `https://xxx.supabase.co` |
| `SUPABASE_KEY` | ClÃ© anonyme Supabase | `eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...` |
| `SUPABASE_SERVICE_KEY` | ClÃ© service role Supabase | `eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...` |
| `DATABASE_URL` | URL de connexion PostgreSQL | `postgresql://postgres:pass@...` |
| `JWT_SECRET` | Secret pour JWT | `your-secret-key` |
| `ETL_SCHEDULE` | Planning ETL (format cron) | `0 */6 * * *` |
| `API_URL` | URL de l'API (pour Streamlit) | `http://api:8000` |

## ğŸ§ª Tests

Des scripts de test sont disponibles dans le dossier `test/` :

```bash
# Test de configuration globale
./test/test_config.sh

# Test de connexion Supabase
python3 test/supabase/test_supabase_connection.py
```

Voir `test/README.md` pour plus de dÃ©tails.

## ğŸ³ Commandes Docker utiles

```bash
# DÃ©marrer tous les services
docker-compose up

# DÃ©marrer en arriÃ¨re-plan
docker-compose up -d

# Voir les logs
docker-compose logs -f

# ArrÃªter les services
docker-compose down

# Reconstruire les images
docker-compose build --no-cache

# RedÃ©marrer un service spÃ©cifique
docker-compose restart <service>

# ExÃ©cuter une commande dans un conteneur
docker-compose exec <service> <command>
```

## ğŸ“Š Supabase

### Configuration de la base de donnÃ©es

1. CrÃ©er les tables nÃ©cessaires dans Supabase
2. Configurer les politiques RLS (Row Level Security)
3. Tester les connexions depuis l'API

### Migration de schÃ©ma

Utilisez l'interface Supabase ou des migrations SQL pour gÃ©rer le schÃ©ma de base de donnÃ©es.

## ğŸ”’ SÃ©curitÃ©

- Les variables d'environnement sensibles ne doivent jamais Ãªtre commitÃ©es
- Utilisez les politiques RLS de Supabase pour la sÃ©curitÃ© des donnÃ©es
- Validez toutes les entrÃ©es utilisateur dans l'API
- Utilisez HTTPS en production

## ğŸ“ˆ AmÃ©liorations futures

- [ ] Ajouter des tests unitaires et d'intÃ©gration
- [ ] ImplÃ©menter l'authentification complÃ¨te
- [ ] Ajouter la gestion des erreurs avancÃ©e
- [ ] Optimiser les performances de l'ETL
- [ ] Ajouter des mÃ©triques et monitoring
- [ ] DÃ©ploiement en production (CI/CD)

## ğŸ‘¥ Contribution

Ce projet est dÃ©veloppÃ© dans le cadre d'un projet EPSI


